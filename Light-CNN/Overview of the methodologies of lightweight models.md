## Overview of the methodologies of lightweight models 

Pruning network connections or channels reduces redundant connections in a pre-trained model with maintaining performance

Quantization and factorization are proposed in literature to reduce redundancy in calculations to speed up inference

Optimized convolution algorithms implemented by FFT and other methods decrease time consumption in practice.

Distilling transfers knowledge from large models into small ones, which makes trainging small models easier.

Using better model designs, depthwise seperable convolution, network architecture.